# Machine Learning

目前计划：

**课程方面** 周一、三、五学习MIT的线性代数 [链接]( https://www.bilibili.com/video/av46288016?p=2 )，周二、四、六学习Berkeley的算法课 [链接]( https://www.youtube.com/watch?v=f7fri6FsvvQ )

**书本方面** 周一、三、五学习图像处理，周二、四、六学习西瓜书，周三、周六学习统计学习方法  [GitHub链接](https://github.com/fengdu78/lihang-code) 

周日完成西瓜书后面的习题，适当完成统计学习方法的代码，查漏补缺并整理报告

每天至少三小时，尽量达到四小时以上，少的时间要尽快补上
		
		
## 2019.12.8 fixed

### GAN：

通过李宏毅的教程详细学习了GAN的数学理论以及各种应用与变种，并且结合实际案例（生成各种种类的图片）编写代码更新到Coding_homework/下，整理了学习笔记到GAN/下，整理了一份报告到GAN/下。

### 算法部分：

强化了动态规划算法的学习，继续往后推进了四节课，已更新笔记。



### 线性代数部分：

继续往后推进了5课，已更新笔记。



### 机器学习（周志华）：

决策树结束。



## 2019.11.6 fixed

###  图像处理部分：
##### 整体进度从3.5学习到了第四章结束。

3.5各向异性扩散涉及到热方程的并没有理解，先mark一下

了解了各种算子的特征（优点以及缺点），**平均算了可以去除大量噪声，但是使特征边界模糊;高斯平均保留更多特征，但与直接求平均相比，几乎没有优势(噪声不是高斯分布的);中值算子保留些噪声，但得到清晰的边界特征;截断中值算子去除更多噪声，但也去除更多图像细节。很显然，由图3.24(b)和图3.24(c)的结果表明，加大截断中值模板可以提高性能。这是预料之中的，由于通过加大截断中值模板，实质上可以加大分布，从中找出模式。**

第四章讲的是低层次特征提取以及边缘检测。最基本的是差分+均匀阈值方法。并且学习了分块阈值方法, 以便每个块的光照都近似均匀的 。[学习链接]( https://blog.csdn.net/kk55guang2/article/details/78475414 ).

接着从数学角度（泰勒展开）理解了边缘检测算子。并学习了更多的边缘检测算子，如Prewitt,  Sobel(帕斯卡三角形), 并从频域角度理解了Sobel算子，复习了z变换。接着学习了Canny算子和滞后阈值。

二阶算子：包括拉普拉斯算子，Marr-Hildreth(高斯算子与图像进行卷积后进行二阶微分，同时包含微分和平滑，这个算子与人类视觉密切关系，具有多分辨率分析能力)。

比较了各种 边缘检测算子： **中值滤波常用于一般性（非超声波）的程序处理，作为*预滤波器*用于一阶和二阶方法效果显著；所有边缘算子的结果都是用滞后阈值处理来实现的，其中，为了更好地反映其性能，这些阈值都是手动选择的。Prewitt 和Sobel 算子显示了一些边界,但处理后的图像中仍然保留了许多噪声（Sobel少一点）；Laplacian 算子对于大噪声的图片几乎没有留下什么信息；Mar- Hildreth方法的处理结果有了一些改善，对于如此大噪声的图像，很难选择个大小适当的LoG算子来检测如此多维的特征，在噪声过滤所需的算子大小与目标特征的大小之间需要折中；Canny 和Spacek算子的处理效果非常好。**

为了进一步学习相位一致性，学习了小波变换，了解了傅立叶变换的弱点以及小波变换的优势。[学习链接](https://www.cnblogs.com/jfdwd/p/9249850.html).

![小波变换](https://img-blog.csdn.net/20160705113512598)

做傅里叶变换只能得到一个频谱，做小波变换却可以得到一个时频谱！

为了学习通过亮度变化计算曲率，学习了图像梯度。[学习链接](https://blog.csdn.net/saltriver/article/details/78987096)



### 机器学习部分：

##### 整体进度从第一章开始一直到第四章第二节结束。

第一章从宏观上把握了机器学习到底是什么，可以用来干什么，发展历史，目前发展到了什么地步，大概是怎么样的一个操作流程。学习了一些机器学习方面的专有名词。根据训练数据是否有标记信息，学习任务大致可以划分为两大类：**监督学习和无监督学习**，**分类和回归**是前者的代表，**聚类**是后者的代表。了解了泛化性、假设空间、归纳偏好等等概念。

第二章开始我觉觉得稍微有点硬核了，数学表达式多了起来，用到了很多以前微积分、概率论、线性代数中的知识，我也因此决定要认认真真补一补数学的基础，包括MIT的线性代数以及统计学习方法。这一章学习了如何评估自己的学习器。我们希望得到泛化误差小的学习器。**然而我们无法直接比较泛化误差**，因此有了这一章的许许多多的评估方法。最基本的是留出法，即按一定比例留出一部分样本当测试集，用测试集来评估误差，作为对泛化误差的估计。接着学习了更加高级有效的交叉验证以及交叉验证的特例，留一法，并编写代码验证了这两种验证的方法。**当数据量不太够的时候**，我们也可以使用自助法。为了调参，还可以在训练数据中分一部分作为验证集：


$$
数据可分为\begin{cases}
训练数据\begin{cases}
训练集:\color{red}{用于训练模型}\\
验证集:\color{red}{用于调参}\\
\end{cases}\\
测试数据:\color{blue}{用于评估模型泛化性}\\
\end{cases}
$$


性能度量最常用的是均方误差，错误率，以及精度。接着就是使我晕头转向的***TP, TN, FP, FN, TPR, FPR, TNR, FNR***这一系列概念以及它们之间的关系，它们可以组合出F1度量，P-R曲线，ROC曲线（纵轴为真正利率TPR，纵轴为假正例率FPR），如何绘制ROC曲线，AUC(ROC曲线下面的面积)，以及如何通过P-R曲线、ROC曲线判断学习器的性能。接着推广到更一般的代价敏感错误率与代价曲线，考虑到FP和FN的代价在现实中并不一样。

之后是基于统计学的比较检验。由于学概率论已经是一年多以前了，因此对 χ2分布 ，t分布，F分布已经有点模糊了，经过好长一段时间资料的搜索以及回忆过后，大概搞明白对于单个学习器来说，可以假设它的泛化性能并通过t分布检验。如果是要对两个不同学习器的性能进行比较，可以使用交叉验证t检验McNemar检验（ χ2分布 检验）以及较为复杂（**其实是还没搞明白**）的Friedman检验。

最后了解了机器学习中泛化误差、偏差与方差的关系：

<img src="https://img-blog.csdn.net/20170607091627872?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvenhkMTc1NDc3MTQ2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="泛化误差、偏差与方差的关系" style="zoom:67%;" />

第三章讲的是线性模型，学习了线性回归，可以通过最小二乘法来求解，**但当属性数超过样例数的时候，此时可能有很多解，常见的作法就是引入正则化项。** 逻辑回归适用于分类的，引入了sigmoid函数，方便将值映射到(0,1)之间，再通过决定阈值（**需要考虑类别不平衡问题进行“再缩放”**），此外，对于*多重分类逻辑回归模型*，需要将sigmoid变成softmax。当然，多分类问题还可以拆成OvO,OvR,MvM以及ECOC编码。ECOC最佳编码是要求任意两个类别之间的编码距离越远越好（**详见作业第六题**）。

逻辑回归的推导运用到了概率论中所讲的极大似然法，当时有点忘了，又上网学习了一下，最大化”对数似然“，即令每个样本属于其真实标记的概率越大越好。接着又是涉及到线性代数的Hessian矩阵（然而我已经记忆模糊，只好上网再学习一番）,通过Hessian矩阵判断是凸函数，可以通过梯度下降法求解。

还有一个在数学上比较硬核一些的LDA(线性判别分析)，理解了它的核心思想，也通过代码实现了， <font color=#FF0000 >只不过推导的最后一部分还有点看不懂，可能是线性代数的知识还不够吧，先mark一下</font> 。



第四章先看了两节，第一节让我对决策树有了一个基本的了解，并从宏观上给了实现决策树的伪代码。第二节就讲如何选择最优划分属性，**我们希望分支节点所包含的样本尽量属于同一类别，即纯度尽可能高**。定义了信息熵，信息熵增益，以及增益率，共同帮助决定按照什么样的顺序展开决策树。最后还对照实际的西瓜例子人工跑了一遍决策树，感觉对决策树的理解得到了巩固。

